name: "DSPy and GEPA Evaluation for GPT-5 and Gemini"
description: "Evaluate DSPy (default) and GEPA optimization performance on question answering task"

models:
  - provider: openai
    model: gpt-5
  - provider: gemini
    model: gemini-2.5-pro

tasks:
  - name: question_answering
    dataset: squad_v2
    metrics: [accuracy, f1_score]
    num_samples: 10

methods:
  dspy:
    method: chain_of_thought
    num_threads: 2
    metrics: [accuracy, f1_score, semantic_similarity]
  
  gepa:
    population_size: 10
    generations: 5
    mutation_rate: 0.3
    auto_budget: "light"
    eval_samples: 5

output:
  format: [json, csv]
  include_plots: false
  save_intermediate: true
  output_dir: "./results/dspy_gepa_evaluation"
